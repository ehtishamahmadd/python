{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name    = \"Ehtisham Ahmad\"\n",
    "\n",
    "email   = \"ehtishamahmed10@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>likeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>170.688</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Biryani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>165.000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Biryani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>171.000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Biryani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>173.000</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Biryani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>164.000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Biryani</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   height  weight  gender likeness\n",
       "0   27  170.688    76.0       1  Biryani\n",
       "1   41  165.000    70.0       1  Biryani\n",
       "2   29  171.000    80.0       1  Biryani\n",
       "3   27  173.000   102.0       1  Biryani\n",
       "4   29  164.000    67.0       1  Biryani"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('mldata.csv')\n",
    "df['gender'] = df['gender'].replace(\"Male\",1)\n",
    "df['gender'] = df['gender'].replace(\"Female\",0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df [['weight','gender']]\n",
    "y = df['likeness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Biryani'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model= KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "predicted= model.predict([[78,1]])\n",
    "predicted\n",
    "\n",
    "#K-NN creates biasness due to more repeating neighboring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score =  0.673469387755102\n"
     ]
    }
   ],
   "source": [
    "# metrcies for evaluation\n",
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "model= KNeighborsClassifier(n_neighbors=6).fit(X_train,y_train)\n",
    "\n",
    "predicted_values = model.predict(X_test)\n",
    "predicted_values\n",
    "\n",
    "score = accuracy_score(y_test, predicted_values)\n",
    "print('accuracy score = ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "# Some important points:\n",
    "\n",
    "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced.\n",
    "\n",
    "A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.\n",
    "\n",
    "A low recall score (<0.5) means your classifier has a high number of False negatives which can be an outcome of imbalanced class.\n",
    "\n",
    "As our recall increases, our precision decreases because, in addition to increasing the true positives, we increase the false positives.\n",
    "\n",
    "Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial.\n",
    "\n",
    "In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on.\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. It takes both false positive and false negatives into account. Therefore, it performs well on an imbalanced dataset. F1 score gives the same weightage to recall and precision.\n",
    "\n",
    "F1 Score becomes 1 only when precision and recall are both 1. F1 score becomes high only when both precision and recall are high.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Score\n",
    "### precision score = ( True Positive / (True positive * False positive) )\n",
    "\n",
    "The ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The best value is 1 and the worst value is 0.\n",
    "\n",
    "average{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None, default=’binary’\n",
    "\n",
    "This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "- 'binary':\n",
    "\n",
    "Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "- 'micro':\n",
    "\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "- 'macro':\n",
    "\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "- 'weighted':\n",
    "\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "- 'samples':\n",
    "\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score(macro) =  0.5912698412698413\n",
      "Precision score (micro) =  0.673469387755102\n",
      "Precision score (weighted) =  0.6345966958211856\n"
     ]
    }
   ],
   "source": [
    "# metrcies for evaluation\n",
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "model= KNeighborsClassifier(n_neighbors=6).fit(X_train,y_train)\n",
    "\n",
    "predicted_values = model.predict(X_test)\n",
    "predicted_values\n",
    "\n",
    "score = precision_score(y_test, predicted_values, average='macro')\n",
    "print('Precision score(macro) = ', score)\n",
    "\n",
    "score1 = precision_score(y_test, predicted_values, average='micro')\n",
    "print('Precision score (micro) = ', score1)\n",
    "\n",
    "score2 = precision_score(y_test, predicted_values, average='weighted')\n",
    "print('Precision score (weighted) = ', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## Recall Score\n",
    "\n",
    "### precision score = ( True Positive / (True positive * False Negative) )\n",
    "\n",
    "Ability of the classifier to find all the positive samples.\n",
    "\n",
    "The best value is 1 and the worst value is 0.\n",
    "\n",
    "average {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None, default=’binary’\n",
    "\n",
    "This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "- 'binary':\n",
    "\n",
    "Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "- 'micro':\n",
    "\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "- 'macro':\n",
    "\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "- 'weighted':\n",
    "\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall. Weighted recall is equal to accuracy.\n",
    "\n",
    "- 'samples':\n",
    "\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score(macro) =  0.4641203703703704\n",
      "Recall score (micro) =  0.673469387755102\n",
      "Recall score (weighted) =  0.673469387755102\n"
     ]
    }
   ],
   "source": [
    "# metrcies for evaluation\n",
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "model= KNeighborsClassifier(n_neighbors=6).fit(X_train,y_train)\n",
    "\n",
    "predicted_values = model.predict(X_test)\n",
    "predicted_values\n",
    "\n",
    "score = recall_score(y_test, predicted_values, average='macro')\n",
    "print('Recall score(macro) = ', score)\n",
    "\n",
    "score1 = recall_score(y_test, predicted_values, average='micro')\n",
    "print('Recall score (micro) = ', score1)\n",
    "\n",
    "score2 = recall_score(y_test, predicted_values, average='weighted')\n",
    "print('Recall score (weighted) = ', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## F-1 Score\n",
    "### F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "\n",
    "average {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None, default=’binary’\n",
    "\n",
    "This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "- 'binary':\n",
    "\n",
    "Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "- 'micro':\n",
    "\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "- 'macro':\n",
    "\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "- 'weighted':\n",
    "\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall. Weighted recall is equal to accuracy.\n",
    "\n",
    "- 'samples':\n",
    "\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score(macro) =  0.4834834834834834\n",
      "F1 score (micro) =  0.673469387755102\n",
      "F1 score (weighted) =  0.6241036955322669\n"
     ]
    }
   ],
   "source": [
    "# metrcies for evaluation\n",
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "model= KNeighborsClassifier(n_neighbors=6).fit(X_train,y_train)\n",
    "\n",
    "predicted_values = model.predict(X_test)\n",
    "predicted_values\n",
    "\n",
    "score = f1_score(y_test, predicted_values, average='macro')\n",
    "print('F1 score(macro) = ', score)\n",
    "\n",
    "score1 = f1_score(y_test, predicted_values, average='micro')\n",
    "print('F1 score (micro) = ', score1)\n",
    "\n",
    "score2 = f1_score(y_test, predicted_values, average='weighted')\n",
    "print('F1 score (weighted) = ', score2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d406864f2ee78ecc67afccccd0a48c216f41ca3a0f9ca739a0b9d8b15ae422ba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
