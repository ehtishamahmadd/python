{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name    = \"Ehtisham Ahmad\"\n",
    "\n",
    "email   = \"ehtishamahmed10@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# Least Square Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best way of fitting a curve on top of a chart of data points.\n",
    "\n",
    "- Widely used to make scatter plots easier to interpret and is associated with regression analysis.\n",
    "\n",
    "- The least-squares method is a form of mathematical regression analysis used to determine the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points.\n",
    "  \n",
    "-  Create a straight line that minimizes the sum of the squares of the errors that are generated by the results of the associated equations, such as the squared residuals resulting from differences in the observed value, and the value anticipated, based on that model. \n",
    "\n",
    "- After plotting points we can generate a regression line of best fit.\n",
    "\n",
    "- It minimizes the vertical distance from the data points to the regression line. The term “least squares” is used because it is the smallest sum of squares of errors, which is also called the \"variance\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Activation Functions:\n",
    "\n",
    "- It tells us how well the network model learns the training dataset. It defines the type of predictions the model can make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Step Function:\n",
    "\n",
    "- The binary step function can be used as an activation function while creating a binary classifier. As you can imagine, this function will not be useful when there are multiple classes in the target variable. That is one of the limitations of binary step function.\n",
    "- Gradient(Slope) of function becomes zero.\n",
    "- No component of x.\n",
    "- f(x) = 1, x>=0\n",
    "- f(x) = 0, x<0\n",
    "\n",
    "## 2. Linear Function:\n",
    "\n",
    "- Gradient(Slope) of function is not zero.\n",
    "- Network will not really improve the error since the gradient is the same for every iteration. The network will not be able to train well and capture the complex patterns from the data. Hence, linear function might be ideal for simple tasks where interpretability is highly desired.\n",
    "- f(x)=ax\n",
    "\n",
    "## 3. Sigmoid Function:\n",
    "\n",
    "-  It is a smooth S-shaped function.\n",
    "-  It is one of the most widely used non-linear activation function. Sigmoid transforms the values between the range 0 and 1.\n",
    "- Unlike the binary step and linear functions, sigmoid is a non-linear function.\n",
    "- Gradient(Slope) is small but not zero.\n",
    "- f(x) = 1/(1+e^-x)\n",
    "\n",
    "## 4. Tanh Function:\n",
    "\n",
    "- The tanh function is very similar to the sigmoid function. The only difference is that it is symmetric around the origin. The range of values in this case is from -1 to 1.\n",
    "- The gradient of the tanh function is steeper as compared to the sigmoid function.\n",
    "- Usually tanh is preferred over the sigmoid function since it is zero centered and the gradients are not restricted to move in a certain direction.\n",
    "- tanh(x) = 2/(1+e^(-2x)) -1\n",
    "\n",
    "## 5. ReLU Function:(Rectiufied Linear Unit)\n",
    "\n",
    "- Non-linear activation function.\n",
    "- Only operates on positive values. For the negative input values, the result is zero.\n",
    "- ReLU function is far more computationally efficient when compared to the sigmoid and tanh function.\n",
    "- f(x)=max(0,x)\n",
    "\n",
    "## 6. Leaky ReLU Function:\n",
    "\n",
    "- Improved version of the ReLU function.\n",
    "- In Leaky ReLU instead of defining the Relu function as 0 for negative values of x, we define it as an extremely small linear component of x.\n",
    "- We dont make negative values 0(dead) in this function.\n",
    "\n",
    "## 7. Parameterised ReLU Function:\n",
    "\n",
    "- This is another variant of ReLU that aims to solve the problem of gradient’s becoming zero for the left half of the axis.\n",
    "- It introduces a new parameter as a slope of the negative part of the function. \n",
    "- It is used when the leaky ReLU function still fails to solve the problem of (-ive) dead values.\n",
    "- f(x) = x, x>=0\n",
    "- f(x) = ax, x<0\n",
    "\n",
    "## 8. Exponential Linear Unit Function:(ELU)\n",
    "\n",
    "- A variant of ReLU that modifies the slope of the negative part of the function.\n",
    "- Unlike the leaky relu and parametric ReLU functions, instead of a straight line, ELU uses a log curve for defning the negatice values.\n",
    "- f(x) = x,   x>=0\n",
    "- f(x)= a(e^x-1), x<0\n",
    "\n",
    "## 9. Swish Function:\n",
    "\n",
    "- Swish is as computationally efficient as ReLU and shows better performance than ReLU on deeper models.\n",
    "- The values for swish ranges from negative infinity to infinity.\n",
    "- It has smooth curve.\n",
    "- The value of the function may decrease even when the input values are increasing.\n",
    "- f(x) = x*sigmoid(x)\n",
    "- f(x) = x/(1-e^-x)\n",
    "\n",
    "## 10. Softmax Function:\n",
    "\n",
    "- Combination of multiple sigmoids.\n",
    "- The softmax function can be used for multiclass classification problems.\n",
    "- This function returns the probability for a datapoint belonging to each individual class.\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the right Activation Function:\n",
    "\n",
    "- Sigmoid functions and their combinations generally work better in the case of classifiers.\n",
    "- Sigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem.\n",
    "- ReLU function is a general activation function and is used in most cases these days.\n",
    "- If we encounter a case of dead values in our networks the leaky ReLU function is the best choice.\n",
    "- Always keep in mind that ReLU function should only be used in the hidden layers.\n",
    "- As a rule of thumb, you can begin with using ReLU function and then move over to other activation functions in case ReLU doesn’t provide with optimum results.\n",
    "\n",
    "----------------"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d406864f2ee78ecc67afccccd0a48c216f41ca3a0f9ca739a0b9d8b15ae422ba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
